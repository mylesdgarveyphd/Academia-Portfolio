{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOxc2rkyBTIJhRuAvo1Tjau",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mylesdgarveyphd/Academia-Portfolio/blob/main/neural_network_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate the Training Data"
      ],
      "metadata": {
        "id": "YfDt_4ziO_gw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X-X15vAL8WcD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Ensure necessary library is installed\n",
        "def setup_environment():\n",
        "    try:\n",
        "        from PIL import Image\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", \"pillow\"])\n",
        "\n",
        "setup_environment()\n",
        "\n",
        "def create_image_with_two(output_dir, image_size=(128, 128), num_images=100):\n",
        "    \"\"\"\n",
        "    Generate images with the concept of \"2\" embedded in them.\n",
        "\n",
        "    Args:\n",
        "        output_dir (str): Directory to save the images.\n",
        "        image_size (tuple): Size of the generated images (width, height).\n",
        "        num_images (int): Number of images to generate.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img = Image.new(\"RGB\", image_size, \"white\")\n",
        "        draw = ImageDraw.Draw(img)\n",
        "\n",
        "        # Randomly decide what kind of \"2\" representation to draw\n",
        "        representation_type = random.choice([\"lines\", \"dots\", \"shapes\"])\n",
        "\n",
        "        if representation_type == \"lines\":\n",
        "            # Draw two random lines\n",
        "            for _ in range(2):\n",
        "                x1, y1 = random.randint(0, image_size[0]), random.randint(0, image_size[1])\n",
        "                x2, y2 = random.randint(0, image_size[0]), random.randint(0, image_size[1])\n",
        "                draw.line((x1, y1, x2, y2), fill=\"black\", width=2)\n",
        "\n",
        "        elif representation_type == \"dots\":\n",
        "            # Draw two random dots\n",
        "            for _ in range(2):\n",
        "                x, y = random.randint(0, image_size[0] - 10), random.randint(0, image_size[1] - 10)\n",
        "                radius = random.randint(3, 10)\n",
        "                draw.ellipse((x, y, x + radius, y + radius), fill=\"black\")\n",
        "\n",
        "        elif representation_type == \"shapes\":\n",
        "            # Draw two random shapes (rectangles, ovals)\n",
        "            for _ in range(2):\n",
        "                x1, y1 = random.randint(0, image_size[0] - 20), random.randint(0, image_size[1] - 20)\n",
        "                x2, y2 = x1 + random.randint(10, 30), y1 + random.randint(10, 30)\n",
        "                shape_type = random.choice([\"rectangle\", \"oval\"])\n",
        "\n",
        "                if shape_type == \"rectangle\":\n",
        "                    draw.rectangle((x1, y1, x2, y2), outline=\"black\", width=2)\n",
        "                elif shape_type == \"oval\":\n",
        "                    draw.ellipse((x1, y1, x2, y2), outline=\"black\", width=2)\n",
        "\n",
        "        # Save the image\n",
        "        img.save(os.path.join(output_dir, f\"image_{i:04d}.png\"))\n",
        "\n",
        "# Example usage\n",
        "output_directory = \"generated_images\"\n",
        "create_image_with_two(output_directory, image_size=(128, 128), num_images=500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Neural Network"
      ],
      "metadata": {
        "id": "z1KxNHcqPHMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Ensure necessary libraries are installed\n",
        "def setup_environment():\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", \"torch\"])\n",
        "setup_environment()\n",
        "\n",
        "class DynamicANN(nn.Module):\n",
        "    def __init__(self, input_size, layer_config, output_size=None):\n",
        "        super(DynamicANN, self).__init__()\n",
        "\n",
        "        output_size = output_size or input_size\n",
        "\n",
        "        layers = []\n",
        "        current_size = input_size\n",
        "\n",
        "        for neurons in layer_config:\n",
        "            layers.append(nn.Linear(current_size, neurons))\n",
        "            layers.append(nn.ReLU())\n",
        "            current_size = neurons\n",
        "\n",
        "        layers.append(nn.Linear(current_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Load and preprocess images\n",
        "def load_and_preprocess_image(image_path, image_size=(128, 128)):\n",
        "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    img = img.resize(image_size)\n",
        "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
        "    return img_array.flatten()  # Flatten for input into ANN\n",
        "\n",
        "# Save the output image\n",
        "def save_output_image(output_tensor, output_path, image_size=(128, 128)):\n",
        "    output_array = output_tensor.detach().numpy().reshape(image_size) * 255.0\n",
        "    output_image = Image.fromarray(output_array.astype(np.uint8))\n",
        "    output_image.save(output_path)\n",
        "\n",
        "# Train the model\n",
        "def train_model(output_dir, layer_config=(3, 2, 6), num_iterations=50000):\n",
        "    image_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.png')]\n",
        "    input_size = 128 * 128\n",
        "    output_size = input_size\n",
        "\n",
        "    # Initialize the neural network with dynamic configuration\n",
        "    model = DynamicANN(input_size, layer_config)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "    param_changes = []\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        # Randomly select two images\n",
        "        im1_path, im2_path = random.sample(image_files, 2)\n",
        "        im1 = load_and_preprocess_image(im1_path)\n",
        "        im2 = load_and_preprocess_image(im2_path)\n",
        "\n",
        "        # Convert to tensors\n",
        "        input_tensor = torch.tensor(im1, dtype=torch.float32).unsqueeze(0)\n",
        "        target_tensor = torch.tensor(im2, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_tensor)\n",
        "\n",
        "        # Compute loss and perform backpropagation\n",
        "        loss = criterion(output, target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track parameter changes\n",
        "        param_change = sum(torch.sum(torch.abs(p.grad)).item() for p in model.parameters() if p.grad is not None)\n",
        "        param_changes.append(param_change)\n",
        "\n",
        "        # Print and save output image for every 1000th iteration\n",
        "        if (iteration + 1) % 1000 == 0:\n",
        "            layer_config_str = '_'.join(map(str, layer_config))\n",
        "            output_filename = f\"out_iter{iteration + 1}_{layer_config_str}.png\"\n",
        "            save_output_image(output, os.path.join(output_dir, output_filename))\n",
        "            print(f\"Iteration {iteration + 1}/{num_iterations}, Loss: {loss.item():.6f}, Param Change: {param_change:.6f}\")\n",
        "            print(f\"Output image saved as {output_filename}\")\n",
        "\n",
        "    # Save final output image\n",
        "    example_input_path = random.choice(image_files)\n",
        "    example_input = load_and_preprocess_image(example_input_path)\n",
        "    example_tensor = torch.tensor(example_input, dtype=torch.float32).unsqueeze(0)\n",
        "    example_output = model(example_tensor)\n",
        "    layer_config_str = '_'.join(map(str, layer_config))\n",
        "    final_output_filename = f\"out_final_{layer_config_str}.png\"\n",
        "    save_output_image(example_output, os.path.join(output_dir, final_output_filename))\n",
        "    print(f\"Final output image saved as {final_output_filename}\")\n",
        "\n",
        "    return param_changes\n",
        "\n",
        "# Example usage\n",
        "output_directory = \"generated_images\"\n",
        "# You can now easily configure the layer structure\n",
        "param_changes = train_model(output_directory, layer_config=(128,128), num_iterations=50000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "woZUntozBsSp",
        "outputId": "71f59dc0-8ac3-40ef-ac30-84408d2241d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1000/50000, Loss: 0.094654, Param Change: 194.713219\n",
            "Output image saved as out_iter1000_128_128.png\n",
            "Iteration 2000/50000, Loss: 0.111953, Param Change: 681.736643\n",
            "Output image saved as out_iter2000_128_128.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b05e8fedcafc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0moutput_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"generated_images\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# You can now easily configure the layer structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mparam_changes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-b05e8fedcafc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(output_dir, layer_config, num_iterations)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Track parameter changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combine visual data into a video"
      ],
      "metadata": {
        "id": "r9pL2JhBPLyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def create_video_from_images(image_dir, output_video_path, frame_rate=5, image_size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Create a video from a sequence of images in a directory.\n",
        "\n",
        "    Args:\n",
        "        image_dir (str): Path to the directory containing images.\n",
        "        output_video_path (str): Path to save the generated video.\n",
        "        frame_rate (int): Frame rate for the video.\n",
        "        image_size (tuple): Size of each image in the video.\n",
        "    \"\"\"\n",
        "    def extract_iteration_number(filename):\n",
        "        try:\n",
        "            # Extract number after 'iter' and before '_'\n",
        "            return int(filename.split('_iter')[1].split('_')[0])\n",
        "        except (IndexError, ValueError):\n",
        "            return float('inf')  # Non-conforming files go to the end\n",
        "\n",
        "    # Get and sort image files\n",
        "    image_files = sorted(\n",
        "        [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('3_2_6.png')],\n",
        "        key=lambda x: extract_iteration_number(x)\n",
        "    )\n",
        "\n",
        "    if not image_files:\n",
        "        print(\"No images found in the directory.\")\n",
        "        return\n",
        "\n",
        "    # Define the codec and create a VideoWriter object\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' for .mp4 files\n",
        "    video_writer = cv2.VideoWriter(output_video_path, fourcc, frame_rate, image_size)\n",
        "\n",
        "    for image_file in image_files:\n",
        "        # Read the image\n",
        "        img = cv2.imread(image_file)\n",
        "        # Resize the image to match the video size\n",
        "        img_resized = cv2.resize(img, image_size)\n",
        "        # Write the frame to the video\n",
        "        video_writer.write(img_resized)\n",
        "\n",
        "    # Release the VideoWriter\n",
        "    video_writer.release()\n",
        "    print(f\"Video saved to {output_video_path}\")\n",
        "\n",
        "# Example usage\n",
        "image_directory = \"generated_images\"  # Directory with your images\n",
        "video_path = \"training_output_video_n300.mp4\"  # Path for saving the video\n",
        "create_video_from_images(image_directory, video_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOPdh6z8NBJO",
        "outputId": "f7ff78bc-19c1-42ac-e6c0-4ead40d91d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved to training_output_video_n300.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sandbox"
      ],
      "metadata": {
        "id": "1In8Of0UeMq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class FastImageDataset(Dataset):\n",
        "    def __init__(self, image_dir):\n",
        "        self.images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize((128, 128)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        img = self.transform(Image.open(img_path))\n",
        "        return img.view(-1)\n",
        "\n",
        "class FastImageGPT(nn.Module):\n",
        "    def __init__(self, input_size, layer_config=(128, 64, 32), num_heads=4):\n",
        "        super().__init__()\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, input_size))\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads),\n",
        "            num_layers=len(layer_config)\n",
        "        )\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_size, layer_config[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer_config[0], input_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pos_embed\n",
        "        x = x.unsqueeze(0)  # Add batch dimension\n",
        "        x = self.transformer(x)\n",
        "        return self.mlp(x.squeeze(0))\n",
        "\n",
        "def train_model(output_dir, layer_config=(128, 64, 32), num_epochs=50):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = FastImageDataset(output_dir)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count())\n",
        "\n",
        "    model = FastImageGPT(input_size=128*128, layer_config=layer_config).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            output = model(batch)\n",
        "            loss = criterion(output, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Usage\n",
        "output_directory = \"generated_images\"\n",
        "model = train_model(output_directory)"
      ],
      "metadata": {
        "id": "NOxfkwtueNbX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}